"""
Agent Base Class / æ™ºèƒ½ä½“åŸºç±»

This module defines the core Agent class that orchestrates LLM interactions
and tool usage.
æ­¤æ¨¡å—å®šä¹‰äº†åè°ƒLLMäº¤äº’å’Œå·¥å…·ä½¿ç”¨çš„æ ¸å¿ƒAgentç±»ã€‚

Author: LLM Agent Framework
License: MIT
"""

import json
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

from .llm_client import LLMClient
from .tool import Tool

# Import prompt templates
try:
    from .prompts import get_system_prompt, format_tools_description, ROLE_TEMPLATES
except ImportError:
    # Fallback if prompts module doesn't exist
    ROLE_TEMPLATES = {}
    def get_system_prompt(*args, **kwargs):
        return ""
    def format_tools_description(tools):
        return ""


class Agent:
    """
    Core agent class that combines LLM and tools to execute tasks.
    ç»“åˆLLMå’Œå·¥å…·æ‰§è¡Œä»»åŠ¡çš„æ ¸å¿ƒæ™ºèƒ½ä½“ç±»ã€‚

    An agent maintains conversation history, manages tools, and implements
    the reasoning loop for tool usage.
    æ™ºèƒ½ä½“ç»´æŠ¤å¯¹è¯å†å²ã€ç®¡ç†å·¥å…·ï¼Œå¹¶å®ç°å·¥å…·ä½¿ç”¨çš„æ¨ç†å¾ªç¯ã€‚

    Attributes:
        name (str): Agent name / æ™ºèƒ½ä½“åç§°
        llm_client (LLMClient): LLM client for API calls / ç”¨äºAPIè°ƒç”¨çš„LLMå®¢æˆ·ç«¯
        tools (List[Tool]): Available tools / å¯ç”¨å·¥å…·
        system_prompt (str): System prompt for the agent / æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤º
        memory_enabled (bool): Whether to maintain conversation history / æ˜¯å¦ç»´æŠ¤å¯¹è¯å†å²
    """

    def __init__(
        self,
        name: str,
        llm_client: LLMClient,
        tools: Optional[List[Tool]] = None,
        system_prompt: Optional[str] = None,
        role: str = "é€šç”¨åŠ©æ‰‹",
        memory_enabled: bool = True,
        max_memory_tokens: int = 4000,
        max_iterations: int = 10,
        use_react: bool = True
    ):
        """
        Initialize the Agent.
        åˆå§‹åŒ–æ™ºèƒ½ä½“ã€‚

        Args:
            name: Agent name / æ™ºèƒ½ä½“åç§°
            llm_client: LLM client instance / LLMå®¢æˆ·ç«¯å®ä¾‹
            tools: List of available tools / å¯ç”¨å·¥å…·åˆ—è¡¨
            system_prompt: Custom system prompt (will be added to role template) / è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºï¼ˆå°†æ·»åŠ åˆ°è§’è‰²æ¨¡æ¿ï¼‰
            role: Agent role type (é€šç”¨åŠ©æ‰‹/æ•°æ®åˆ†æå¸ˆ/æ•°å­¦è€å¸ˆ/etc.) / æ™ºèƒ½ä½“è§’è‰²ç±»å‹
            memory_enabled: Enable conversation memory / å¯ç”¨å¯¹è¯è®°å¿†
            max_memory_tokens: Max tokens for memory / è®°å¿†çš„æœ€å¤§ä»¤ç‰Œæ•°
            max_iterations: Max reasoning iterations / æœ€å¤§æ¨ç†è¿­ä»£æ¬¡æ•°
            use_react: Use ReAct reasoning template / ä½¿ç”¨ReActæ¨ç†æ¨¡æ¿
        """
        self.name = name
        self.llm_client = llm_client
        self.tools = {tool.name: tool for tool in (tools or [])}
        self.role = role
        self.use_react = use_react
        self.custom_instructions = system_prompt or ""
        
        # Generate system prompt using template
        tools_list = list(self.tools.values())
        if tools_list and 'get_system_prompt' in globals():
            tools_desc = format_tools_description(tools_list)
            self.system_prompt = get_system_prompt(
                tools_description=tools_desc,
                role=role,
                custom_instructions=self.custom_instructions,
                use_react=use_react
            )
        else:
            self.system_prompt = self._default_system_prompt()
        
        self.memory_enabled = memory_enabled
        self.max_memory_tokens = max_memory_tokens
        self.max_iterations = max_iterations

        self.conversation_history: List[Dict[str, str]] = []
        self.execution_log: List[Dict[str, Any]] = []

    def _default_system_prompt(self) -> str:
        """
        Generate default system prompt with tool descriptions.
        ç”ŸæˆåŒ…å«å·¥å…·æè¿°çš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚

        Returns:
            System prompt string / ç³»ç»Ÿæç¤ºå­—ç¬¦ä¸²
        """
        base_prompt = f"""You are {self.name}, a helpful AI assistant.

You have access to the following tools:
"""

        if self.tools:
            for tool_name, tool in self.tools.items():
                base_prompt += f"\n- {tool_name}: {tool.description}"

            base_prompt += """

To use a tool, respond with a JSON object in this format:
```json
{
    "tool": "tool_name",
    "parameters": {
        "param1": "value1",
        "param2": "value2"
    }
}
```

After receiving tool results, continue the conversation or use another tool if needed.
If you don't need any tools, respond normally to the user.
"""
        else:
            base_prompt += "\nNo tools available. Respond based on your knowledge."

        return base_prompt

    def run_stream(
        self,
        task: str,
        context: Optional[Dict[str, Any]] = None,
        callback=None
    ):
        """
        Execute a task using the agent with streaming output.
        ä½¿ç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡å¹¶æµå¼è¾“å‡ºã€‚
        
        This method yields updates as they happen, allowing for real-time display.
        æ­¤æ–¹æ³•åœ¨æ›´æ–°å‘ç”Ÿæ—¶äº§ç”Ÿå®ƒä»¬ï¼Œå…è®¸å®æ—¶æ˜¾ç¤ºã€‚
        
        Args:
            task: Task description / ä»»åŠ¡æè¿°
            context: Additional context / é¢å¤–ä¸Šä¸‹æ–‡
            callback: Optional callback function(event_type, content) / å¯é€‰çš„å›è°ƒå‡½æ•°
            
        Yields:
            Dict with event type and content / åŒ…å«äº‹ä»¶ç±»å‹å’Œå†…å®¹çš„å­—å…¸
        """
        messages = self._prepare_messages(task, context)
        
        iteration = 0
        while iteration < self.max_iterations:
            iteration += 1
            
            yield {
                "type": "iteration",
                "iteration": iteration,
                "content": f"ğŸ”„ ç¬¬ {iteration} è½®æ¨ç†..."
            }
            
            # Stream LLM response
            response = self.llm_client.chat(messages, stream=True)
            
            if not response.get("success"):
                error_msg = f"LLM API error: {response.get('error')}"
                yield {"type": "error", "content": error_msg}
                return
            
            # Collect streaming content
            full_content = ""
            if response.get("stream"):
                yield {"type": "thought_start", "content": "ğŸ’­ æ€è€ƒä¸­: "}
                for chunk in self.llm_client.parse_stream(response["response"]):
                    full_content += chunk
                    yield {"type": "thought_chunk", "content": chunk}
                yield {"type": "thought_end", "content": "\n"}
            else:
                full_content = response["content"]
                yield {"type": "thought", "content": f"ğŸ’­ æ€è€ƒ: {full_content}\n"}
            
            self._log_execution("llm_response", full_content)
            
            # Try to extract final_answer first
            final_answer = self._extract_final_answer(full_content)
            if final_answer:
                if self.memory_enabled:
                    self.conversation_history.append({
                        "role": "user",
                        "content": task
                    })
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": final_answer
                    })
                
                yield {"type": "final_answer", "content": f"\nâœ… æœ€ç»ˆç­”æ¡ˆ: {final_answer}"}
                return
            
            # Try to parse tool call
            tool_call = self._parse_tool_call(full_content)
            
            if tool_call:
                # Show tool execution
                tool_name = tool_call.get("tool", "unknown")
                params = tool_call.get("parameters", {})
                
                yield {
                    "type": "tool_call",
                    "content": f"ğŸ› ï¸  è°ƒç”¨å·¥å…·: {tool_name}\n   å‚æ•°: {json.dumps(params, ensure_ascii=False)}\n"
                }
                
                # Execute the tool
                tool_result = self._execute_tool(tool_call)
                
                yield {
                    "type": "tool_result",
                    "content": f"ğŸ“Š å·¥å…·ç»“æœ: {json.dumps(tool_result, ensure_ascii=False)}\n"
                }
                
                # Add to messages
                messages.append({
                    "role": "assistant",
                    "content": full_content
                })
                
                observation = f"Observation: {json.dumps(tool_result, ensure_ascii=False)}"
                messages.append({
                    "role": "user",
                    "content": observation
                })
                
                if self.memory_enabled:
                    self.conversation_history.extend([
                        {"role": "assistant", "content": full_content},
                        {"role": "user", "content": observation}
                    ])
            else:
                # No tool call and no final_answer
                if self.memory_enabled:
                    self.conversation_history.append({
                        "role": "user",
                        "content": task
                    })
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": full_content
                    })
                
                yield {"type": "response", "content": full_content}
                return
        
        yield {"type": "max_iterations", "content": "âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"}

    def run(
        self,
        task: str,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Execute a task using the agent.
        ä½¿ç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡ã€‚

        This is the main entry point for agent execution. It handles the
        reasoning loop, tool calling, and response generation.
        è¿™æ˜¯æ™ºèƒ½ä½“æ‰§è¡Œçš„ä¸»è¦å…¥å£ç‚¹ã€‚å®ƒå¤„ç†æ¨ç†å¾ªç¯ã€å·¥å…·è°ƒç”¨å’Œå“åº”ç”Ÿæˆã€‚

        Args:
            task: Task description / ä»»åŠ¡æè¿°
            context: Additional context / é¢å¤–ä¸Šä¸‹æ–‡

        Returns:
            Final response string / æœ€ç»ˆå“åº”å­—ç¬¦ä¸²
        """
        messages = self._prepare_messages(task, context)

        iteration = 0
        while iteration < self.max_iterations:
            iteration += 1

            response = self.llm_client.chat(messages)

            if not response.get("success"):
                error_msg = f"LLM API error: {response.get('error')}"
                self._log_execution("error", error_msg)
                return error_msg

            content = response["content"]
            self._log_execution("llm_response", content)
            
            # Try to extract final_answer first
            final_answer = self._extract_final_answer(content)
            if final_answer:
                # This is the final answer, return it
                if self.memory_enabled:
                    self.conversation_history.append({
                        "role": "user",
                        "content": task
                    })
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": final_answer
                    })
                return final_answer

            # Try to parse tool call
            tool_call = self._parse_tool_call(content)

            if tool_call:
                # Execute the tool
                tool_result = self._execute_tool(tool_call)

                # Add assistant's reasoning and tool call to messages
                messages.append({
                    "role": "assistant",
                    "content": content
                })
                
                # Add tool result as observation
                observation = f"Observation: {json.dumps(tool_result, ensure_ascii=False)}"
                messages.append({
                    "role": "user",
                    "content": observation
                })

                if self.memory_enabled:
                    self.conversation_history.extend([
                        {"role": "assistant", "content": content},
                        {"role": "user", "content": observation}
                    ])
            else:
                # No tool call and no final_answer, treat as direct response
                if self.memory_enabled:
                    self.conversation_history.append({
                        "role": "user",
                        "content": task
                    })
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": content
                    })

                return content

        return "Maximum iterations reached. Task may be incomplete."
    
    def _extract_final_answer(self, content: str) -> Optional[str]:
        """
        Extract final answer from ReAct format response.
        ä»ReActæ ¼å¼å“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆã€‚
        
        Args:
            content: LLM response content
            
        Returns:
            Final answer string or None
        """
        # Try to extract from JSON
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
        
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                if "final_answer" in data:
                    return data["final_answer"]
            except json.JSONDecodeError:
                pass
        
        # Try to parse entire content as JSON
        try:
            data = json.loads(content)
            if "final_answer" in data:
                return data["final_answer"]
        except (json.JSONDecodeError, TypeError):
            pass
        
        return None

    def _prepare_messages(
        self,
        task: str,
        context: Optional[Dict[str, Any]]
    ) -> List[Dict[str, str]]:
        """
        Prepare message list for LLM API call.
        ä¸ºLLM APIè°ƒç”¨å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ã€‚

        Args:
            task: Task description / ä»»åŠ¡æè¿°
            context: Additional context / é¢å¤–ä¸Šä¸‹æ–‡

        Returns:
            List of message dicts / æ¶ˆæ¯å­—å…¸åˆ—è¡¨
        """
        messages = [{"role": "system", "content": self.system_prompt}]

        if self.memory_enabled and self.conversation_history:
            messages.extend(self.conversation_history[-10:])

        if context:
            context_str = f"Additional context: {json.dumps(context)}\n\n"
            task = context_str + task

        messages.append({"role": "user", "content": task})

        return messages

    def _parse_tool_call(self, content: str) -> Optional[Dict[str, Any]]:
        """
        Parse tool call from LLM response.
        ä»LLMå“åº”ä¸­è§£æå·¥å…·è°ƒç”¨ã€‚
        
        Supports both old format (tool/parameters) and new ReAct format (action/action_input).
        æ”¯æŒæ—§æ ¼å¼ï¼ˆtool/parametersï¼‰å’Œæ–°ReActæ ¼å¼ï¼ˆaction/action_inputï¼‰ã€‚

        Args:
            content: LLM response content / LLMå“åº”å†…å®¹

        Returns:
            Tool call dict or None / å·¥å…·è°ƒç”¨å­—å…¸æˆ–None
        """
        # Try to extract JSON from markdown code block
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)

        if json_match:
            try:
                data = json.loads(json_match.group(1))
                
                # Check for final_answer (end of reasoning)
                if "final_answer" in data:
                    return None  # No tool call, this is the final answer
                
                # New ReAct format: action/action_input
                if "action" in data and "action_input" in data:
                    return {
                        "tool": data["action"],
                        "parameters": data["action_input"]
                    }
                
                # Old format: tool/parameters
                if "tool" in data:
                    return data
                    
            except json.JSONDecodeError:
                pass

        # Try to parse the entire content as JSON
        try:
            data = json.loads(content)
            
            # Check for final_answer
            if "final_answer" in data:
                return None
            
            # New ReAct format
            if "action" in data and "action_input" in data:
                return {
                    "tool": data["action"],
                    "parameters": data["action_input"]
                }
            
            # Old format
            if "tool" in data:
                return data
                
        except (json.JSONDecodeError, TypeError):
            pass

        return None

    def _execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute a tool based on the parsed tool call.
        æ ¹æ®è§£æçš„å·¥å…·è°ƒç”¨æ‰§è¡Œå·¥å…·ã€‚

        Args:
            tool_call: Tool call specification / å·¥å…·è°ƒç”¨è§„èŒƒ

        Returns:
            Tool execution result / å·¥å…·æ‰§è¡Œç»“æœ
        """
        tool_name = tool_call.get("tool")
        parameters = tool_call.get("parameters", {})

        self._log_execution("tool_call", {
            "tool": tool_name,
            "parameters": parameters
        })

        if tool_name not in self.tools:
            result = {
                "success": False,
                "error": f"Tool '{tool_name}' not found"
            }
            self._log_execution("tool_error", result)
            return result

        tool = self.tools[tool_name]

        try:
            result = tool.execute(**parameters)
            self._log_execution("tool_result", result)
            return result
        except Exception as e:
            result = {
                "success": False,
                "error": str(e)
            }
            self._log_execution("tool_error", result)
            return result

    def _log_execution(self, event_type: str, data: Any) -> None:
        """
        Log execution events for debugging and analysis.
        è®°å½•æ‰§è¡Œäº‹ä»¶ä»¥è¿›è¡Œè°ƒè¯•å’Œåˆ†æã€‚

        Args:
            event_type: Type of event / äº‹ä»¶ç±»å‹
            data: Event data / äº‹ä»¶æ•°æ®
        """
        self.execution_log.append({
            "timestamp": datetime.now().isoformat(),
            "event_type": event_type,
            "data": data
        })

    def add_tool(self, tool: Tool) -> None:
        """
        Add a tool to the agent.
        å‘æ™ºèƒ½ä½“æ·»åŠ å·¥å…·ã€‚

        Args:
            tool: Tool instance / å·¥å…·å®ä¾‹
        """
        self.tools[tool.name] = tool
        self.system_prompt = self._default_system_prompt()

    def remove_tool(self, tool_name: str) -> bool:
        """
        Remove a tool from the agent.
        ä»æ™ºèƒ½ä½“ä¸­ç§»é™¤å·¥å…·ã€‚

        Args:
            tool_name: Name of tool to remove / è¦ç§»é™¤çš„å·¥å…·åç§°

        Returns:
            True if removed, False if not found / å¦‚æœå·²ç§»é™¤åˆ™ä¸ºTrue
        """
        if tool_name in self.tools:
            del self.tools[tool_name]
            self.system_prompt = self._default_system_prompt()
            return True
        return False

    def clear_memory(self) -> None:
        """
        Clear conversation history.
        æ¸…é™¤å¯¹è¯å†å²ã€‚
        """
        self.conversation_history.clear()

    def get_execution_log(self) -> List[Dict[str, Any]]:
        """
        Get the execution log.
        è·å–æ‰§è¡Œæ—¥å¿—ã€‚

        Returns:
            List of execution events / æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        return self.execution_log.copy()

    def clear_execution_log(self) -> None:
        """
        Clear the execution log.
        æ¸…é™¤æ‰§è¡Œæ—¥å¿—ã€‚
        """
        self.execution_log.clear()

    def __repr__(self) -> str:
        """String representation of the agent."""
        return f"Agent(name='{self.name}', tools={list(self.tools.keys())})"
